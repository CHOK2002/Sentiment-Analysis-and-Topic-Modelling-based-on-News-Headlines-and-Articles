{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Data (Balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check each category consist how many data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "news             18047\n",
       "sport            15719\n",
       "politics          3026\n",
       "business          1774\n",
       "health            1035\n",
       "world              820\n",
       "entertainment      556\n",
       "us                 317\n",
       "opinions           280\n",
       "weather            114\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the data \n",
    "file_path = \"cnn_news_articles_final_cleaned.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data = data[['text','category']]\n",
    "\n",
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category of World and Entertainment aug 1 time with Back Translate technique (Too heavy) - Aborted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlpaug.augmenter.word as naw\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# import os\n",
    "# import nltk\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # load data\n",
    "# file_path = \"Cleaned_News_Articles_Final.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "# data = data[data['category'].isin(['world', 'entertainment'])]\n",
    "# data = data[['text', 'category']]\n",
    "\n",
    "# # minority category\n",
    "# minority_labels = {\n",
    "#     \"world\": 820,\n",
    "#     \"entertainment\": 562,\n",
    "# }\n",
    "\n",
    "# # using back translate techique to aug\n",
    "# back_translation_aug = naw.BackTranslationAug(\n",
    "#     from_model_name='Helsinki-NLP/opus-mt-en-zh',\n",
    "#     to_model_name='Helsinki-NLP/opus-mt-zh-en')\n",
    "\n",
    "# # lists to store augmented data\n",
    "# augmented_summaries = []\n",
    "# multilabels = []\n",
    "\n",
    "# for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "#     if isinstance(row['category'], str):\n",
    "#         augmented_labels = row['category'].split(',')  \n",
    "#         augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "#         if augmented_labels_filtered:\n",
    "#             # augment the summary for in text\n",
    "#             augmented_summary_backtranslate = back_translation_aug.augment(row['text'])\n",
    "\n",
    "#             # append to the lists\n",
    "#             augmented_summaries.append(augmented_summary_backtranslate)\n",
    "#             multilabels.append(row['category'])\n",
    "\n",
    "# augmented_df = pd.DataFrame({'text': augmented_summaries, 'category': multilabels})\n",
    "\n",
    "# data_augmented_path = 'test.csv'\n",
    "# augmented_df.to_csv(data_augmented_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category of World and Entertainment aug 1 time with 1 technique (using synonym techique to aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe Chok\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 41688/41688 [00:27<00:00, 1533.55it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# minority category\n",
    "minority_labels = {\n",
    "    \"world\": 820,\n",
    "    \"entertainment\": 556,\n",
    "}\n",
    "\n",
    "# using synonym techique to aug\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# lists to store augmented data\n",
    "augmented_summaries = []\n",
    "multilabels = []\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    if isinstance(row['category'], str):\n",
    "        augmented_labels = row['category'].split(',')  \n",
    "        augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "        if augmented_labels_filtered:\n",
    "            # augment the summary for in text\n",
    "            augmented_summary_synonym = aug_synonym.augment(row['text'])\n",
    "\n",
    "            # append to the lists\n",
    "            augmented_summaries.append(augmented_summary_synonym)\n",
    "            multilabels.extend([row['category']] * 1)\n",
    "\n",
    "augmented_df = pd.DataFrame({'text': augmented_summaries, 'category': multilabels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check total of augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "world            820\n",
       "entertainment    556\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category of US, Opinions and Waether aug 4 times with 4 different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe Chok\\anaconda3\\envs\\fyp\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Joe Chok\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Joe Chok\\anaconda3\\envs\\fyp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joe Chok\\anaconda3\\envs\\fyp\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Joe Chok\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "100%|██████████| 41688/41688 [2:01:06<00:00,  5.74it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# minority category\n",
    "minority_labels = {\n",
    "    \"us\": 317,\n",
    "    \"opinions\": 280,\n",
    "    \"weather\": 114,\n",
    "}\n",
    "\n",
    "# techniques to augmented\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"substitute\")\n",
    "\n",
    "aug2 = naw.ContextualWordEmbsAug(\n",
    "    model_path='roberta-base', action=\"substitute\")\n",
    "\n",
    "aug_insert = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', action=\"insert\")\n",
    "\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# lists to store augmented data\n",
    "augmented_summaries = []\n",
    "multilabels = []\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    if isinstance(row['category'], str):\n",
    "        augmented_labels = row['category'].split(',') \n",
    "        augmented_labels_filtered = [label.strip() for label in augmented_labels if label.strip() in minority_labels]\n",
    "        \n",
    "        if augmented_labels_filtered:\n",
    "            # augment the summary for in headline\n",
    "            augmented_summary = aug.augment(row['text'])\n",
    "            augmented_summary2 = aug2.augment(row['text'])\n",
    "            augmented_summary_insert = aug_insert.augment(row['text'])\n",
    "            augmented_summary_synonym = aug_synonym.augment(row['text'])\n",
    "            \n",
    "            # append to the lists\n",
    "            augmented_summaries.extend([augmented_summary, augmented_summary2, augmented_summary_insert, augmented_summary_synonym])\n",
    "            multilabels.extend([row['category']] * 4)\n",
    "\n",
    "augmented_df2 = pd.DataFrame({'text': augmented_summaries, 'category': multilabels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check total of augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "us          1268\n",
       "opinions    1120\n",
       "weather      456\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df2['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both Augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[promise second cold war impact portrayal russ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[actor mila raised million refugee state amid ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[west year due concerning behavior three origi...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[punk band anti war spin clash hit song callin...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[version story pop life chronicle weekly enter...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category\n",
       "0  [promise second cold war impact portrayal russ...  entertainment\n",
       "1  [actor mila raised million refugee state amid ...  entertainment\n",
       "2  [west year due concerning behavior three origi...  entertainment\n",
       "3  [punk band anti war spin clash hit song callin...  entertainment\n",
       "4  [version story pop life chronicle weekly enter...  entertainment"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two DataFrames\n",
    "combined_aug_data = pd.concat([augmented_df, augmented_df2], ignore_index=True)\n",
    "combined_aug_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Augmented data & without Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[promise second cold war impact portrayal russ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[actor mila raised million refugee state amid ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[west year due concerning behavior three origi...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[punk band anti war spin clash hit song callin...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[version story pop life chronicle weekly enter...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category\n",
       "0  [promise second cold war impact portrayal russ...  entertainment\n",
       "1  [actor mila raised million refugee state amid ...  entertainment\n",
       "2  [west year due concerning behavior three origi...  entertainment\n",
       "3  [punk band anti war spin clash hit song callin...  entertainment\n",
       "4  [version story pop life chronicle weekly enter...  entertainment"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two DataFrames\n",
    "combined_aug_and_without_aug_data = pd.concat([combined_aug_data, data], ignore_index=True)\n",
    "combined_aug_and_without_aug_data.head()\n",
    "\n",
    "final = \"cnn_news_articles_final_augmented_cleaned.csv\"\n",
    "combined_aug_and_without_aug_data.to_csv(final, index=False)\n",
    "combined_aug_and_without_aug_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove square brackets, single quotes, and double quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>promise second cold war impact portrayal russi...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor mila raised million refugee state amid o...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>west year due concerning behavior three origin...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>punk band anti war spin clash hit song calling...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>version story pop life chronicle weekly entert...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category\n",
       "0  promise second cold war impact portrayal russi...  entertainment\n",
       "1  actor mila raised million refugee state amid o...  entertainment\n",
       "2  west year due concerning behavior three origin...  entertainment\n",
       "3  punk band anti war spin clash hit song calling...  entertainment\n",
       "4  version story pop life chronicle weekly entert...  entertainment"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'cnn_news_articles_final_augmented_cleaned.csv'\n",
    "output_file = 'cnn_news_articles_final_augmented_cleaned.csv'\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# remove square brackets, single quotes, double quotes\n",
    "df['text'] = df['text'].str.replace(r\"[\\[\\]\\\"']\", '', regex=True)\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling to select each category 1000 data (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe Chok\\AppData\\Local\\Temp\\ipykernel_13520\\1917975080.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  downsampled_data = df_downsampled.groupby('category').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "business         1000\n",
       "entertainment    1000\n",
       "health           1000\n",
       "news             1000\n",
       "opinions         1000\n",
       "politics         1000\n",
       "sport            1000\n",
       "us               1000\n",
       "world            1000\n",
       "weather           570\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df_downsampled = pd.read_csv('cnn_news_articles_final_augmented_cleaned.csv')\n",
    "\n",
    "# select 10000 data for each sentiment\n",
    "lower_limit = 100\n",
    "upper_limit = 1000\n",
    "\n",
    "# grp the data by sentiment and perform downsampling within each grp\n",
    "downsampled_data = df_downsampled.groupby('category').apply(lambda x: x.sample(n=min(upper_limit, max(lower_limit, len(x))), random_state=42))\n",
    "\n",
    "downsampled_data = downsampled_data.reset_index(drop=True)\n",
    "\n",
    "downsampled_data.to_csv('cnn_news_articles_final_downsampled_cleaned.csv', index=False)\n",
    "downsampled_data['category'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
